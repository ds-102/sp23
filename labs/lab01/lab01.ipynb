{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab01.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1:  Basics of Testing\n",
    "Welcome to the first Data 102 lab! \n",
    "\n",
    "The goals of this lab are to get familiar with concepts in decision theory. We will learn more about testing, p-values and FDR control.\n",
    "\n",
    "The code you need to write is commented out with a message **\"TODO: fill...\"**. There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Submission\n",
    "To submit this assignment, follow the instructions as outlined in the [Gradescope Submission Guidelines post](https://edstem.org/us/courses/33922/discussion/2419862) on Ed.\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted by Wednesday, February 1st, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Let's begin by importing the libraries we will use. You can find the documentation for the libraries here:\n",
    "* matplotlib: https://matplotlib.org/3.1.1/contents.html\n",
    "* numpy: https://docs.scipy.org/doc/\n",
    "* pandas: https://pandas.pydata.org/pandas-docs/stable/\n",
    "* seaborn: https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import hashlib\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Hypothesis testing, LRT, decision rules, P-values.\n",
    "\n",
    "The first question looks at the basics of testing. You will have to put yourself in the shoes of a detective who is trying to use 'evidence' to find the 'truth'. Given a piece of evidence $X$ your job will be to decide between two hypotheses. The two hypothesis you consider are:\n",
    "\n",
    "_The null hypothesis:_\n",
    "$$ H_0: X\\sim \\mathcal{N}(0,1)$$\n",
    "_The alternative hypothesis:_\n",
    "$$ H_1: X \\sim \\mathcal{N}(2,1)$$\n",
    "\n",
    "Granted you don't know the truth, but you have to make a decision that maximizes the True Positive Probability and minimizes the False Positive Probability.\n",
    "\n",
    "**In this exercise you will look at:**\n",
    " - The intuitive relationship between Likelihood Ratio Test and decisions based on thresholding $X$.\n",
    " - The performance of a level-$\\alpha$ test.\n",
    " - The distribution of p-values for samples from the null distribution as well as samples from the alternative.\n",
    " \n",
    "#### Let's start by plotting the distributions of the null and alternative hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you just need to run this cell to plot the pdf; don't change this code.\n",
    "def null_pdf(x):\n",
    "    return norm.pdf(x,0,1)\n",
    "def alt_pdf(x):\n",
    "    return norm.pdf(x,2,1)\n",
    "\n",
    "# Plot the distribution under the null and alternative\n",
    "x_axis = np.arange(-4, 6, 0.001)\n",
    "\n",
    "plt.plot(x_axis, null_pdf(x_axis), label = '$H_0$') # <- likelihood under the null\n",
    "plt.fill_between(x_axis, null_pdf(x_axis), alpha = 0.3)\n",
    "\n",
    "plt.plot(x_axis, alt_pdf(x_axis),  label = '$H_1$') # <- likelihood alternative\n",
    "plt.fill_between(x_axis, alt_pdf(x_axis), alpha = 0.3)\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.title(\"Comparison of null and alternative likelihoods\");\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the image above we can see that if the data lies towards the right, then it seems more plausible that the alternative is true. For example $X\\geq1.64$ seems much less likely to belong to the null pdf than the alternative pdf.\n",
    "\n",
    "### Likelihood Ratio Test\n",
    "In class we said that the optimal test is the Likelihood Ratio Test (LRT), which is the result of the celebrated Neyman-Pearson Lemma. It says that the optimal level $\\alpha$ test is the one that rejects the null (aka makes a discovery, favors the alternative) whenever:\n",
    "$$LR(x):=\\frac{f_1(x)}{f_0(x)} \\geq \\eta$$\n",
    "where $\\eta$ is chosen such that the false positive rate is equal to $\\alpha$.\n",
    "\n",
    "### But how does this result fit with the intuition that we should set a decision threshold based on the value of $X$ directly?\n",
    "\n",
    "This exercise will formalize that intuition:\n",
    "\n",
    "Let's start by computing the ratio of the likelihoods. The likelihood of $X\\sim \\mathcal N(\\mu,\\sigma)$ is:\n",
    "$$f_{\\sigma, \\mu}(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Luckily `scipy` has a nifty function to compute the likelihood of gaussians `scipy.norm.pdf(x, mu, sigma)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.a:  Calculate likelihood ratios\n",
    "\n",
    "Complete the function below that computes the likelihood ratio for any value `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: fill in the missing expression for the likelihood ratio in the function below\n",
    "def calculate_likelihood_ratio(x):\n",
    "    \"\"\"\n",
    "    Computes the likelihood ratio between the alternative and null hypothesis.\n",
    "    \n",
    "    Inputs:\n",
    "        x: value for which to compute the likelihood ratio\n",
    "    \n",
    "    Outputs:\n",
    "        lr : the likelihood ratio at point x\n",
    "    \"\"\"\n",
    "    \n",
    "    L0 = null_pdf(x)\n",
    "    L1 = alt_pdf(x)\n",
    "    LR = ... # TODO: fill the likelihood ratio\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's plot the likelihood ratios for different values of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the likelihood ratio for X=1.64\n",
    "X = 1.64\n",
    "LR = calculate_likelihood_ratio(X)\n",
    "print(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below plots the LR for different values of X\n",
    "# Once you've filled in `calculate_likelihood_ratio` run this cell and inspect the plot\n",
    "x_axis = np.arange(-1, 3, 0.001)\n",
    "plt.plot(x_axis, calculate_likelihood_ratio(x_axis))\n",
    "plt.vlines(X, 0, LR, linestyle=\"dotted\", color='k')\n",
    "plt.hlines(LR, -1, X, linestyle=\"dotted\", color='k')\n",
    "plt.scatter(X, LR, 30, color='k')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Likelihood Ratio\")\n",
    "plt.title(\"Comparison of null and alternative likelihoods\");\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above illustrates that deciding based on LRT with $\\eta = 3.6$ (the dotted horizontal line) is equivalent to deciding in the favor of the alternative whenever $X\\geq 1.64$ (the dotted vertical line). The set $[1.64, +\\infty)$ is called the **rejection region** of the test, because for all X values in the rejection region the test rejects the null in the favor of the alternative. This illustrates that our intuition was correct.\n",
    "\n",
    "When thinking in terms of likelihood ratios it seems very tricky to compute the False Positive Rate (FPR), however in this case we can bypass that by testing based on the value of $X$.\n",
    "\n",
    "#### The figure below illustrates pictorially the FPR when testing based on the threshold $X\\geq1.64$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(-4, 5, 0.001)\n",
    "\n",
    "\n",
    "plt.plot(x_axis, null_pdf(x_axis), label = '$H_0$') # <- likelihood under the null\n",
    "plt.plot(x_axis, alt_pdf(x_axis),  label = '$H_1$') # <- likelihood alternative\n",
    "\n",
    "rejection_region = np.arange(X, 5, 0.001) # <- truncate the true rejection region for plotting purposes\n",
    "plt.fill_between(rejection_region, null_pdf(rejection_region), alpha = 0.3, label=\"FPR\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.title(\"Comparison of null and alternative likelihoods\");\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis it is still possible to observe values in the tail of distribution. The probability of that happening is exactly FPR (and it is illustrated by the shaded area under the null curve). Assume that we are using the test that rejects for all $X \\geq \\tau$ (in the example above, $\\tau = 1.64$). FPR can then be computed as:\n",
    "\n",
    "$$FPR(\\tau) = \\mathbb{P}\\{X>\\tau|H_0 \\text{ is true}\\} = 1 - \\mathbb{P}\\{X<\\tau|H_0 \\text{ is true}\\} = 1 - F(\n",
    "\\tau)$$\n",
    "\n",
    "where $F(\\cdot)$ denotes the CDF of the null distributions, which in this case is the standard gaussian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.b: Calculate the probability of False Positives\n",
    "In the cell below calculate the FPR for this test. \n",
    "\n",
    "**Hint**, the cdf of a standard normal might come in handy for this function: `scipy.stats.norm.cdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: fill in the missing expression for FPR\n",
    "def calculate_fpr(tau):\n",
    "    \"\"\"\n",
    "    Calculates the FPR for the test based on thresholding X>=tau.\n",
    "    It assumes that the null distribution is the standard gaussian N(0,1)\n",
    "    \n",
    "    Inputs:\n",
    "        tau: test threshold\n",
    "    \n",
    "    Outputs:\n",
    "        fpr: false positive rate\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr = ...\n",
    "    return fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now compute the FPR for a test that rejects whenever $X\\geq 1.64$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the false positive rate\n",
    "X = 1.64\n",
    "fpr = calculate_fpr(X)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like we got really lucky!!! The threshold we choose has a FPR of $\\sim 5\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.c: Make a level-$\\alpha$ decision rule\n",
    "\n",
    "Complete the function below to create a decision rule that has $FPR=\\alpha$. Given that we are working with Gaussian this test is exactly the Z-score test that you might have learned in previous classes.\n",
    "\n",
    "**Hint**, the inverse cdf of a standard normal might come in handy for this function: `scipy.stats.norm.ppf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete this function\n",
    "def make_decision(X, alpha):\n",
    "    \"\"\"\n",
    "    Makes a decision whether to reject the null hypothesis for point X at level alpha\n",
    "    \n",
    "    Inputs:\n",
    "        X: point at which to test\n",
    "        alpha: desired FPR rate for the decision rule (also known as significance level)\n",
    "    \n",
    "    Outputs:\n",
    "        decision: {0, 1} or {False, True}\n",
    "            1/True: reject the null\n",
    "            0/False: fail to reject the null\n",
    "    \"\"\"\n",
    "    threshold = ... # TODO: compute the threshold for which the FPR of the test is equal to alpha (see Hint)\n",
    "    decision = ... # TODO: compute the decision; 1 stands for rejecting the null\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've filled `make_decision` run this to perform this test \n",
    "# for a few values of X at different levels alpha\n",
    "X_vals = np.array([0, 0.5, 1, 2, 3])\n",
    "alphas = np.array([0.01, 0.05, 0.1, 0.2])\n",
    "for alpha in alphas:\n",
    "    decisions = make_decision(X_vals, alpha)\n",
    "    print(f'At FPR={alpha} the null hypothesis is rejected for these X values: {X_vals[decisions==1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.d: Compute P-values\n",
    "Let's take a step back and look at what have we accomplished. We came up with a decision rule that rejects the null hypothesis for a piece of evidence $X$. The test is parametrized at a level $\\alpha$ chosen a-priori to reflect our aversion to False Positives.\n",
    "\n",
    "However, testing returns a binary output: _Reject_ or _Fail to reject_ (1 or 0). In the example above, at level $\\alpha = 0.01$ we reject the null only for $X=3$, however at level $\\alpha = 0.05$ we reject the null for $X=2$ as well. We have already seen that increasing the FPR increases the rejection region of the test. However you might wonder for $X=2$, what is the smallest $\\alpha$ level, such that the corresponding test rejects the null hypothesis in the favor of the alternative?\n",
    "\n",
    "P-values try to answer exactly that question: \n",
    "\n",
    "**\"Given a point $X$, and a family of tests parametrized by $\\alpha$, what is the smallest $\\alpha$ for which the test rejects the null?\"**\n",
    "\n",
    "$$p(X) = \\min_{\\alpha}: Decision_{\\alpha}(X) = 1$$\n",
    "\n",
    "Hence, P-values tell us something more than just a binary accept/reject answer. The P-value associated with the point $X$ quantifies the *strength of the evidence in the favor of rejecting the null*. Small P-values suggest that the evidence is significant, while large P-values suggest that there is little evidence.\n",
    "\n",
    "### In the cell below write a function that computes the P-values, for a point $X$. \n",
    "\n",
    "**Hint**: You already wrote that function in one of the previous exercises, it just had a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete this function\n",
    "def calculate_p_value(X):\n",
    "    \"\"\"\n",
    "    Calculates the P-values for the point X\n",
    "    \n",
    "    Inputs:\n",
    "        X: data point\n",
    "    \n",
    "    Outputs:\n",
    "        p_value: P(X)\n",
    "    \"\"\"\n",
    "    p_value = ...\n",
    "    return(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've filled `calculate_p_value`, run this to compute P-Values for a few X samples.\n",
    "X_vals = np.array([0, 0.5, 1, 2, 3])\n",
    "for X in X_vals:\n",
    "    print(f'X = {X}, P(X) = {calculate_p_value(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Part 1.e: Distribution of p-values\n",
    "Now, we are going to imagine that we have a bunch of samples (each drawn either from the null distribution or the alternative distribution). We want to predict whether each sample was generated from $H_0$ or $H_1$ by looking at its p-value. As a reminder, the two hypothesis to consider are:\n",
    "\n",
    "_The null hypothesis:_\n",
    "$$ H_0: X\\sim \\mathcal{N}(0,1)$$\n",
    "_The alternative hypothesis:_\n",
    "$$ H_1: X \\sim \\mathcal{N}(2,1)$$\n",
    "\n",
    "Assume there are $n=10000$ draws, approximatively 80% of which are nulls (Reality = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you just need to run this cell to instantiate variables; don't change this code.\n",
    "\n",
    "rs = np.random.RandomState(0)\n",
    "n = 10000\n",
    "\n",
    "# roughly 80% of the data comes from the null distribution\n",
    "# true_values is an n-dimensional array of indicators, where \"1\" means that x is from the alternative \n",
    "true_values = rs.binomial(1, 0.2, n)\n",
    "\n",
    "# null distribution is N(0, 1) and alternative distribution is N(2, 1)\n",
    "x_obs = rs.randn(n) + 2*true_values\n",
    "\n",
    "sns.histplot(x_obs[np.where(true_values == 0)],  label=\"samples from null $H_0$ distribution\", kde=False, color='orange')\n",
    "sns.histplot(x_obs[np.where(true_values == 1)],  label=\"samples from alt. $H_1$ distribution\", kde=False, color='blue')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.legend(bbox_to_anchor=(1,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you just need to run this cell and understand what it does; no code to modify or write here. \n",
    "# calculate the p-values for each individual hypothesis\n",
    "p_values = calculate_p_value(x_obs)\n",
    "\n",
    "bins = np.linspace(0,1,num=20)\n",
    "sns.histplot(p_values[np.where(true_values == 0)],  label=\"samples from null $H_0$ distribution\", kde=False, bins=bins, color='orange')\n",
    "sns.histplot(p_values[np.where(true_values == 1)],  label=\"samples from alt. $H_1$ distribution\", kde=False, bins=bins)\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.xlabel(\"p-value\")\n",
    "plt.ylabel(\"frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fill in (<=2 sentences) of your observations. Your answer should connect what you see in the graph with the ideas we've been talking about in class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 2: Multiple Testing - Procedures to control false discovery rate.\n",
    "\n",
    "In the previous example we looked primarily at controling row-wise quantities. And specifically we came up with a decision rules that controls the false positive rate to a desired level $\\alpha$. \n",
    "\n",
    "Now we are switching perspectives and are thiking about a column-wise quantity. Our goal is to control the probability of false discoveries in this decision-making process for multiple hypothesis testing.\n",
    "\n",
    "We will use three methods for making discoveries:\n",
    "\n",
    "    1. Naive thresholding (ignoring that multiple testing is happening)\n",
    "    2. Using Bonferroni correction to account for multiple testing\n",
    "    3. The Benjamini-Hochberg procedure for multiple testing\n",
    "\n",
    "    \n",
    "For each method, we will assess the decisions made on a simulated data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.a: Fill in the following functions regarding confusion matrices.\n",
    "\n",
    "These functions will be important for reporting your results in a standardized way; later code assumes that you have implemented them so start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete this function\n",
    "def report_results(predicted_discoveries, truth):\n",
    "    \"\"\"\n",
    "    Produces a dictionary with counts for the true positives, true negatives,\n",
    "    false negatives, and false positives from the input `predicted_discoveries`\n",
    "    and `truth` arrays.\n",
    "    \n",
    "    Inputs:\n",
    "      predicted discoveries: array of 0/1 values where 1 indicates a \"discovery\".\n",
    "      truth: array of 0/1 values where 1 indicates a draw from the alternative.\n",
    "    \n",
    "    Outputs: a dictionary of TN, TP, FN, and FP counts.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # populate the following dictionary with counts (NOT rates)\n",
    "    # TODO: fill in each of these counts\n",
    "    TP_count = ...\n",
    "    TN_count = ...\n",
    "    FP_count = ...\n",
    "    FN_count = ...\n",
    "    \n",
    "    results_dictionary = {\"TN_count\": TN_count,\n",
    "                          \"TP_count\": TP_count,\n",
    "                          \"FN_count\": FN_count,\n",
    "                          \"FP_count\": FP_count,\n",
    "                         }\n",
    "    \n",
    "    # this function is defined for you below\n",
    "    print_confusion_matrix(results_dictionary)\n",
    "    return results_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete this function\n",
    "def print_false_discovery_fraction(results_dictionary):\n",
    "    total_predicted_discoveries = ...\n",
    "    false_predicted_discoveries = ...\n",
    "    \n",
    "    # TODO: fill in - compute the false discovery fraction from the `results` dictionary\n",
    "    false_discovery_frac = ...\n",
    "    \n",
    "    print(\"total discoveries: {0}\".format(total_predicted_discoveries))\n",
    "    print(\"fraction of discoveries which were actually false: {0:.3f}\".format(false_discovery_frac))\n",
    "    return total_predicted_discoveries, false_discovery_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(res_dict):\n",
    "    # This is a helper function to print the confusion matrix. You don't need to modify this code.\n",
    "    results_df = pd.DataFrame(data = {\"Decision = 0\": [res_dict['TN_count'], res_dict['FN_count']], \n",
    "                                      \"Decision = 1\":  [res_dict['FP_count'], res_dict['TP_count']]},\n",
    "                             index=[\"Truth = 0\", \"Truth = 1\"])\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check, run the cell below to check your functions on a toy example. \n",
    "\n",
    "**Note**: While the autograder will check the correctness of these results, we recommend that you try working out the TP, FP, TN, and TP counts and false discovery fraction in this toy example for yourself before checking your code against the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "toy_truth = np.append(np.zeros(3), np.ones(7)) \n",
    "toy_predicted_discoveries = np.array([0, 1, 1, 0, 0, 0, 1, 1, 1, 1]) \n",
    "toy_dict = report_results(toy_predicted_discoveries, toy_truth)\n",
    "toy_false_discovery_frac = print_false_discovery_fraction(toy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.b: Naive thresholding\n",
    "Here we will investigate the result of using the threshold $\\alpha = 0.05$ to test each hypothesis independently, ignoring that we are in a multiple testing scenario. \n",
    "\n",
    "Fill in the code for the function below to test each hypothesis at significance level $\\alpha$.\n",
    "\n",
    "**Hint** this is very simular to the `make_decision` function you wrote in Problem 1. There the input to the test was the sample value $X$, however here the input is P-value: $p(X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: calculate decisions based on thresholding\n",
    "def naive_alpha_threshold(p_values, alpha):\n",
    "    \"\"\"\n",
    "    Returns decisions on p-values using naive (uncorrected) thresholding.\n",
    "    \n",
    "    Inputs:\n",
    "        p_values: array of p-values\n",
    "        alpha: threshold (significance level)\n",
    "    \n",
    "    Returns:\n",
    "        decisions: binary array of same length as p-values, where `decisions[i]` is 1\n",
    "        if `p_values[i]` is deemed significant at level `alpha`, and 0 otherwize\n",
    "    \"\"\"\n",
    "    \n",
    "    decisions = ...\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've filled in `naive_alpha_threshold`, run this cell to print the results.\n",
    "# set alpha \n",
    "alpha = 0.05\n",
    "\n",
    "# Using the p-values from Part 1.e, we compute the decision according to the naive function\n",
    "naive_decisions = naive_alpha_threshold(p_values, alpha)\n",
    "\n",
    "naive_results = report_results(naive_decisions,true_values)\n",
    "print()\n",
    "print_false_discovery_fraction(naive_results)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.c: Bonferroni Correction\n",
    "Here we will investigate the result of using Bonferroni-corrected p-values to declare discoveries.\n",
    "First, implement the Bonferroni procedure in the function below. \n",
    "\n",
    "Recall that for testing $n$ hypotheses with family-wise error rate (FWER) $\\leq \\alpha$, the resulting procedure is to test each hypothesis with significance $\\frac{\\alpha}{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: calculate the decisions based on the bonferroni correction procedure.\n",
    "def bonferroni(p_values, alpha_total):\n",
    "    \"\"\"\n",
    "    Returns decisions on p-values using the Bonferroni correction.\n",
    "    \n",
    "    Inputs:\n",
    "        p_values: array of p-values\n",
    "        alpha_total: desired family-wise error rate (FWER = P(at least one false discovery))\n",
    "    \n",
    "    Returns:\n",
    "        decisions: binary array of same length as p-values, where `decisions[i]` is 1\n",
    "        if `p_values[i]` is deemed significant, and 0 otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    decisions = ...\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've filled in `bonferroni`, run this cell to print the results. \n",
    "bonferroni_decisions = bonferroni(p_values, alpha)\n",
    "\n",
    "bonferroni_results = report_results(bonferroni_decisions,true_values)\n",
    "print()\n",
    "print_false_discovery_fraction(bonferroni_results)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.d: Benjamini-Hochberg\n",
    "Now we will investigate the result of implementing Benjamini-Hochberg procedure for multiple hypothesis testing.\n",
    "First, implement the Benjamini-Hochberg procedure in the function below. \n",
    "\n",
    "Recall that for testing $n$ hypotheses with false discovery rate (FDR) $\\leq \\alpha$, the resulting procedure is to find the largest p-value less than or equal to $k \\frac{\\alpha}{n}$, where $k$ is given by the p-value's index after sorting, and $n$ represents the number of hypotheses you've tested. Once found, we then declare a discovery for all p-values with value less than or equal to this p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: calculate decisions based on Benjamini-Hochberg procedure\n",
    "def benjamini_hochberg(p_values, alpha):\n",
    "    \"\"\"\n",
    "    Returns decisions on p-values using Benjamini-Hochberg.\n",
    "    \n",
    "    Inputs:\n",
    "        p_values: array of p-values\n",
    "        alpha: desired FDR (FDR = E[# false positives / # positives])\n",
    "    \n",
    "    Returns:\n",
    "        decisions: binary array of same length as p-values, where `decisions[i]` is 1\n",
    "        if `p_values[i]` is deemed significant, and 0 otherwise\n",
    "    \"\"\"\n",
    "    decisions = ...\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assess the result of applying the Benjamini Hochberg procedure to the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've filled in `benjamini_hochberg`, run this cell to print the results.\n",
    "bh_decisions = benjamini_hochberg(p_values, alpha)\n",
    "\n",
    "bh_results = report_results(bh_decisions,true_values)\n",
    "print()\n",
    "\n",
    "print_false_discovery_fraction(bh_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Part 2.e: Conclusions\n",
    "Finally, write a short (<= 4 sentences) summary comparing the three different methods from this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(calculate_likelihood_ratio(1.64), 3.59663972556928)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert calculate_fpr(0) == 0.5\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_fpr(0.5), 0.3085375387259869)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_fpr(1), 0.15865525393145707)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_fpr(2), 0.02275013194817921)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_fpr(3), 0.0013498980316301035)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import itertools\n>>> def test_decisions(make_decision):\n...     answers = [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n...     case_number = 0\n...     for alpha in alphas:\n...         for X_val in X_vals:\n...             assert make_decision(X_val, alpha) == answers[case_number]\n...             case_number += 1\n>>> \n>>> test_decisions(make_decision)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert calculate_p_value(0) == 0.5\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_p_value(0.5), 0.3085375387259869)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_p_value(1), 0.15865525393145707)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_p_value(2), 0.02275013194817921)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(calculate_p_value(3), 0.0013498980316301035)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert toy_dict['TN_count'] == 1\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert toy_dict['FP_count'] == 2\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert toy_dict['FN_count'] == 3\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert toy_dict['TP_count'] == 4\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert toy_false_discovery_frac[0] == 6\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert toy_false_discovery_frac[1] == 1/3\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(naive_results['TP_count']) == 1293\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(naive_results['FP_count']) == 400\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(naive_results['TN_count']) == 7628\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(naive_results['FN_count']) == 679\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(bonferroni_results['TP_count']) == 15\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bonferroni_results['FP_count']) == 0\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bonferroni_results['TN_count']) == 8028\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bonferroni_results['FN_count']) == 1957\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(bh_results['TP_count']) == 391\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bh_results['FP_count']) == 17\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bh_results['TN_count']) == 8011\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(bh_results['FN_count']) == 1581\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
