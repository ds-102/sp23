{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a1c95",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab03.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfa876",
   "metadata": {},
   "source": [
    "# Lab 3:  Bayesian Estimation in Hierarchical Graphical Models\n",
    "Welcome to the third Data 102 lab! \n",
    "\n",
    "The goal of this lab is to go over Bayesian Estimation and provide an introduction to Hierarchial Graphical Models.\n",
    "\n",
    "The code and responses you need to fill in are represented by `...`. There is additional documentation for each part as you go along. \n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Submission\n",
    "See the [Gradescope Submission Guidelines](https://edstem.org/us/courses/33922/discussion/2419862) for details on how to submit your lab.\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, February 15th, 2023 at 11:59 PM PST.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912aae0",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2212179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import beta, binom\n",
    "import itertools\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "\n",
    "import hashlib\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import pymc3 as pm\n",
    "import logging\n",
    "logger = logging.getLogger('pymc3')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252c63",
   "metadata": {},
   "source": [
    "## Question 1: Beta-Binomial Graphical Model\n",
    "\n",
    "In this question we will look at the COVID modeling example. Here's the summary of what you need to know:\n",
    "\n",
    "In this problem we are trying to estimate the COVID infection risk in households. To do that we curate a list of K studies. Each study has an associated pair $(N_i, X_i)$ where $N_i$ denotes the number of susceptible individuals considered and $X_i$ is the number of them that became infected. In our modeling assumptions we assume that each susceptible person gets infected with probability $\\theta_i$. In epidemiology, this quantity is known as Secondary Attack Rate, or SAR for short.\n",
    "\n",
    "We're trying to do two things: \n",
    "1. We want to *combine* the information from all the studies, so we can get a better estimate of SAR than we would with any individual study on its own. \n",
    "2. We want to understand why the studies got different results: specifically, we'd like to figure out the regions with the *lowest* SAR, so that we can investigate what contributed to their relative success. In the other direction, we want to know which regions had the *highest* SAR, since they're likely the ones most urgently in need of intervention measures to help slow the spread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out a dataset \n",
    "study_df = pd.read_csv(\"study_df.csv\", header=0)\n",
    "study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96750e0d",
   "metadata": {},
   "source": [
    "### 1.a Compute the trivial estimate of SAR\n",
    "\n",
    "\n",
    "The most straightforward way to estimate the probability of infection (SAR) is to divide the number of infected cases by the number of susceptible cases. \n",
    "\n",
    "Compute this quantity in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f1e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the function\n",
    "def trivial_theta_estimate(N_value, X_value):\n",
    "    \"\"\"\n",
    "    Computes the trivial estimate of the Secondary Attack Rate\n",
    "    \n",
    "    Inputs:\n",
    "        N_value : int, number of susceptible individuals\n",
    "        X_value : int, number of infected individuals\n",
    "        \n",
    "    Output:\n",
    "        theta_est : float, estimate of probability of infection (SAR)\n",
    "    \"\"\"\n",
    "    theta_est = ...\n",
    "    return theta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8bbb4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply  'trivial_theta_estimate' to each row of the dataframe and add a new column to hold the result\n",
    "study_df['Trivial estimate'] = study_df.apply(lambda row: trivial_theta_estimate(row['N'], row['X']), axis =1)\n",
    "study_df.sort_values('Trivial estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd57951",
   "metadata": {},
   "source": [
    "**Trivial estimates suggest that both minimum and maximum probabilities of infection correspond to small studies.**\n",
    "\n",
    "|      | Min     | Max     |\n",
    "|------|---------|---------|\n",
    "| Name | Study 1 | Study 2 |\n",
    "| X    | 2       | 8       | \n",
    "| N    | 11      | 12      |\n",
    "|$\\theta$| 0.18  | 0.50    |\n",
    "\n",
    "\n",
    "Intuitively, we probably shouldn't be making policy decision based on such small studies alone, especially when this dataset has other studies with tens or even hundreds of people. We would like to balance between strong evidence from the small studies and high confidence in estimates from larger studies.\n",
    "\n",
    "Bayesian inference provides a flexible framework to balance our a priori beliefs with new evidence. Consider the following graphical model:\n",
    "\n",
    "\n",
    "![](model.png)\n",
    "\n",
    "\n",
    "The circles represent random variables, and shaded circles represent observed random variables. The diamond at the top represents fixed, unknown parameters . You'll also see people draw dots or squares for these: there isn't really one consistent notation.\n",
    "\n",
    "Here are a few important quantities in Bayesian inference. This lingo will be used at length in this course and in anything you'll learn in the future about Bayesian inference, so make sure you get familiar with it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4910ebc",
   "metadata": {},
   "source": [
    "### Joint Density / Joint Distribution:\n",
    "The structure of the graphical model specified the full joint density of the parameters and data in the model. For this example the join density is:\n",
    "$$p(\\theta_1, \\theta_2, \\ldots, \\theta_K, X_1, \\ldots, X_K) = \\prod_{\\text{vertex $V$ in graph}}p(V|\\text{parent of $V$}) = \\prod_{i=1}^K \\underbrace{p(\\theta_i|\\alpha, \\beta)}_{\\text{prior of $\\theta_i$}} \\prod_{i=1}^K \\underbrace{p(X_i|\\theta_i)}_{\\text{likelihood of data $X_i$}}$$\n",
    "\n",
    "The factorization of the joint density into products of priors and likelihoods is the key feature of Hierarchical Models. It allows to take a complex $2K$-dimensional joint probability and factorize it into products of 1-dimensional probabilities. This factorization is useful because it lets us simplify the distribution and control the amount of computation we have to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75151cad",
   "metadata": {},
   "source": [
    "### Prior:  $\\theta_i \\sim Beta(\\alpha, \\beta)$\n",
    "\n",
    "We have the prior distribution:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta_i) \n",
    "    &= \\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\\\\n",
    "    &\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}\n",
    "\\end{align}\n",
    "\n",
    "where $\\Gamma$ is the [gamma function](https://en.wikipedia.org/wiki/Gamma_function). Since $\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}$ does not depend on the value of $\\theta$. It is a scaling factor that ensures that $p(\\theta_i)$ is a valid probability function. This leads to a common notation in practice: $p(\\theta_i)\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}$. The symbol $\\propto_{\\theta_i}$ means proportional in $\\theta_i$. This is a little more explicit than the $\\propto$ notation that you usually see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7d958",
   "metadata": {},
   "source": [
    "### Likelihood: $X_i|\\theta \\sim Binomial(N_i, \\theta_i)$\n",
    "\n",
    "We'll use the notation $p(X_i|\\theta)$ for the likelihood function, which represents our belief about the distribution of the data if we know what the parameter $\\theta$ is (in other words, if we condition on $\\theta$).\n",
    "$$p(X_i|\\theta_i) = \\binom{N_i}{X_i} \\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb1c03",
   "metadata": {},
   "source": [
    "### Marginal: Unconditional distribution of $X_i$:\n",
    "\n",
    "\\begin{align}\n",
    "p(X_i)\n",
    "    &= \\int_{\\theta_i} \\overbrace{p(X_i, \\theta_i)}^{\\text{joint distribution}} \\\\\n",
    "    &= \\int_0^1 \\underbrace{p(X_i|\\theta_i)}_{\\text{likelihood}} \\,\\underbrace{p(\\theta_i)}_{\\text{prior}} \\,d\\theta_i\n",
    "\\end{align}\n",
    "\n",
    "This is the marginal distribution over the data: we can plug in a particular set of $X_i$ values and get out the probability that our model assigns to those values, averaged over all possible values of $\\theta$.\n",
    "\n",
    "When formulating a model, we usually choose the prior and the likelihood based on what we know about the problem. This means that computing this marginal distribution over $X_i$ requires *marginalizing* over the parameter $\\theta$: that involves either a summation or an integral (in this case it's an integral because $\\theta$ is continuous)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1272c",
   "metadata": {},
   "source": [
    "### Posterior: $\\theta_i|X_i$\n",
    "The goal of many estimation problems is to obtain the posterior distribution of the parameter of interest $\\theta_i$ conditioned on the data $X_i$.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta_i|X_i) &= \\frac{p(X_i|\\theta_i)p(\\theta_i)}{p(X_i)} \\quad \\text{(by Bayes Rule)}\\\\\n",
    "&\\propto_{\\theta} p(X_i|\\theta_i)p(\\theta_i) \\quad \\text{(the data marginal $p(X_i)$ does not depend on $\\theta$)}\\\\\n",
    "&\\propto_{\\theta}  \\underbrace{\\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}}_{\\text{likelihood}} \\underbrace{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}_{\\text{prior}}\\\\\n",
    "&\\propto_{\\theta}\\theta_i^{\\alpha + X_i - 1}(1-\\theta_i)^{\\beta + N_i - X_i - 1} \\quad \\text{unnormalized Beta density}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1d89b",
   "metadata": {},
   "source": [
    "### Hence $\\theta_i|X_i \\sim Beta(\\alpha + X_i, \\beta + N_i - X_i)$\n",
    "\n",
    "\n",
    "The fact that the posterior probability comes from the same distribution family as the prior is known as *conjugacy*. It is a very useful property because it allows us to compute the posteriors in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4402977",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.b Conceptual\n",
    "1.b 'When specifying a Bayesian model, we use our domain knowledge to establish certain distributions, and then we use computation to find other ones. Which of the following do we establish using our domain knowledge? Pick all that apply.\n",
    "\n",
    "(a) Prior\n",
    "\n",
    "(b) Likelihood\n",
    "\n",
    "(c) Marginal distribution of the data\n",
    "\n",
    "(d) Posterior "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308f610",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df954fdb",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 1.c Examine the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea49aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(alpha_value, beta_value):\n",
    "    x = np.arange(0, 1.01, 0.01)\n",
    "    y = beta.pdf(x, alpha_value, beta_value)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(r'$\\theta_i$')\n",
    "    plt.ylabel(r'$p(\\theta_i)$')\n",
    "    plt.title(r'Beta distribution with parameters $\\alpha$ and $\\beta$')\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_beta, alpha_value=(1, 20, 0.5), beta_value=(1,20, 0.5))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5faaf",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.c (i) Fix `alpha_value = 5`, and experiment with different values of `beta_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15391311",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4185c83",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.c (ii) Fix `beta_value = 5`, and experiment with different values of `alpha_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ecfe7",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1b0c6",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.c (iii) Set `alpha_value = beta_value = 1`, increase their value such that `alpha_value=beta_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0edef",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5bc1c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 1.d Compute Posterior Mean Estimates for SAR\n",
    "In Problem 1 of Discussion 3 we showed that the **posterior mean** minimizes the **Bayes Risk** for the **Squared Error Loss**.\n",
    "\n",
    "#### In the cell below write a function that computes the posterior mean corresponding to $\\theta_i|X_i$.\n",
    "\n",
    "*Hint: If you need to look up facts about certain well-known distributions, you can always (a) go to textbooks from classes you've taken before, (b) look on Wikipedia, or (c) do a simple web search.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a2b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def posterior_mean_estimate(N_value, X_value, alpha_value, beta_value):\n",
    "    \"\"\"\n",
    "    Computes the posterior E[theta_i|X_i] when we consider a prior theta_i ~ Beta(alpha, beta)\n",
    "    \n",
    "    Inputs: \n",
    "        N_value : int, total number of susceptible individuals\n",
    "        X_value : int, number of individuals that became infected\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "    \"\"\"\n",
    "    posterior_mean = ...\n",
    "    return posterior_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d92dda",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff645e83",
   "metadata": {},
   "source": [
    "### 1.e Examine the posterior mean estimate\n",
    "\n",
    "Let's assume that from domain knowledge, we think that the probability of infection (SAR) is close to $\\frac{1}{3}$. We pick a prior distribution for $\\theta_i$ that has mean $\\frac{1}{3}$. Any distribution of the form $\\theta_i \\sim Beta(k, 2k)$ has this property. The value of $k$ determines the 'strength' of the prior. Low values of $k$  correspond to 'flatter' priors, while larger values of $k$ correspond to 'peakier' priors. Play with the sliders in **1.b** to convince yourself.\n",
    "\n",
    "**Examine the plotting function below and answer the qualitative questions in the next cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10538564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify: Examine the code\n",
    "def plot_thetas(k):\n",
    "    \n",
    "    study_df[\"bayesian_theta\"] = study_df.apply(\n",
    "        lambda row: posterior_mean_estimate(row['N'], row['X'], k, 2*k), \n",
    "        axis=1\n",
    "    )\n",
    "    study_df[\"trivial_theta\"] = study_df.apply(\n",
    "        lambda row: trivial_theta_estimate(row['N'], row['X']), \n",
    "        axis=1\n",
    "    )\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    graph = sns.scatterplot(\n",
    "        x=\"trivial_theta\", y=\"bayesian_theta\", \n",
    "        data=study_df, size=\"N\", sizes=(50, 300), alpha=.8\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x='trivial_theta', y='trivial_theta', \n",
    "        data= study_df, ls=\"--\", color='black', lw=1\n",
    "    )\n",
    "    plt.ylim(0.16, 0.52)\n",
    "    graph.axhline(\n",
    "        1/3, color='black', \n",
    "        label = \"$\\frac{1}{3}$ Prior Expectation\"\n",
    "    )\n",
    "    plt.xlabel('Trivial Estimate')\n",
    "    plt.ylabel('Posterior Mean Estimate')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    x = np.arange(0,1.01,0.01)\n",
    "    y = beta.pdf(x, k, 2*k)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(r'$\\theta_i$')\n",
    "    plt.ylabel(r'$p(\\theta_i)$')\n",
    "    plt.title(rf'Prior: $Beta(\\alpha={k}, \\beta={2*k})$')\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_thetas, k=(0, 50, 2))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540fa06",
   "metadata": {},
   "source": [
    "In the plot above the solid horizontal line represents the prior mean estimate $\\mathbb{E}[\\theta_i] = \\frac{k}{k+2k} = 1/3$. The dashed diagonal line marks $x=y$. Each data-point corresponds to a study, the size of the marker denotes the number of susceptible individuals in each study. Such that larger markers correspond to larger studies.\n",
    "\n",
    "**Answer the following questions with 1-2 sentences each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e340f6",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.e (i) Set $k=0$, what do you notice about the data points? Increase steadily the value of $k$. What happens with the points above the solid horizontal line? What about the points below it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8c7cc",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c96ed",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.e (ii) As you increase $k$, which points move faster, larger or slower ones? How can you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc16033",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49445b6",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.e (iii) Imagine that we let $k\\to \\infty$. How do you think the two graphs above will look in the limit $k\\to \\infty$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72847c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7529fb4",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1.e (iv) Fill in the blank in this sentence with either \"small\" or \"large\", and explain your answer: \n",
    "\n",
    "*If we're very sure that the true SAR is close to $\\frac{1}{3}$, we should choose a _______ value of $k$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84179829",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03144560",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Computational Approximate Inference\n",
    "\n",
    "In the previous question we looked at a Beta-Binomial Graphical model. We took advantage of the conjugacy properties of the model and were able to compute closed form solutions for the posterior mean estimates.\n",
    "\n",
    "However, as we introduce more complexity to the model, the conjugacy property quickly breaks and we have to resort to approximate inference. In this class, we'll focus primarily on *sampling* for approximate inference: this will be the topic of the next few lectures and next week's labs. In sampling-based approaches, we don't even try to get the exact posterior: instead, we generate a bunch of samples from it, and use those to approximate the distribution.\n",
    "\n",
    "In this question you will get a taste for probabilistic programming using `PyMC3`. Spend some time perusing the [documentation](https://docs.pymc.io/), but don't worry if there are parts that don't make sense yet. The Quickstart guide is a useful starting point. \n",
    "\n",
    "We'll be using PyMC3 to run an algorithm called Markov Chain Monte Carlo (MCMC), which you'll learn about this week. We'll start by using the same model from Q.1, and compare the results from MCMC with the exact solutions we calculated above. Then, we'll add an extra parameter to the model and make things more complex: even though we can no longer compute our posterior in closed form, MCMC will still generate samples that we can use to estimate each $\\theta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model so that one-time initialization\n",
    "# happens while you're reading over the code in the next cell.\n",
    "\n",
    "# Note: this and the following cells may take a while to run\n",
    "\n",
    "# You can ignore the output of this cell.\n",
    "\n",
    "with pm.Model() as model:\n",
    "    dummy = pm.Beta('dummy', alpha=1, beta=1)\n",
    "    pm.sample(1, return_inferencedata=False, progressbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0d871",
   "metadata": {},
   "source": [
    "## 2.a: Building and sampling from a PyMC3 model\n",
    "\n",
    "The code in the cell below defines the model from Question 1 using PyMC3. Define the theta and X nodes according to the instructions. The PyMC3 website has documentation for defining variables according to the Beta and Binomial distribution. If you have no idea how to proceed, the PyMC3 Quickstart guide or the [notes from lecture 8](http://data102.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/ds-102/sp23&subPath=lecture/lecture08/rejection_sampling.ipynb) are great places to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03febe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def approximate_inference_MCMC(\n",
    "    alpha_value, beta_value, study_df = study_df\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and generates samples from a PyMC3 model of\n",
    "    the posterior distribution that corresponds to the\n",
    "    graphical model in Q.1, using Markov Chain Monte Carlo (MCMC)\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of\n",
    "        the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "        model is a PyMC3 model object, which represents the graphical model\n",
    "        trace is a PyMC3 trace object, which represents 2000 samples\n",
    "            of everything from the posterior\n",
    "    \"\"\"\n",
    "    # Defines the graphical model\n",
    "    with pm.Model() as model:\n",
    "        # The prior for theta is a Beta distribution with parameters\n",
    "        # alpha and beta, and there's one for each study.\n",
    "        # Make sure to name this parameter 'theta' so so this lab can reference it later. \n",
    "        theta = ...\n",
    "        \n",
    "        # The likelihood for X is binomial, with parameter p=theta,\n",
    "        # observed counts in study_df['X'], and observed N similarly\n",
    "        X = ...\n",
    "        \n",
    "        # Generate samples from the posterior distribution using : run 4\n",
    "        # Markov chains of sampling in parallel, generating 500 samples\n",
    "        # each.\n",
    "        trace = pm.sample(500, chains=4, tune=1000, target_accept=0.95, return_inferencedata=False, progressbar=False)\n",
    "    \n",
    "    return (model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527b482",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272ab07",
   "metadata": {},
   "source": [
    "The following cell is an example of how to run the sampler with fixed values of the hyperparameters alpha and beta. **Note that the output is slightly different from what you saw in lecture: it's a little simpler and easier to work with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run approximate inference\n",
    "model, trace = approximate_inference_MCMC(10, 20)\n",
    "\n",
    "# Get posterior samples of theta\n",
    "thetas = trace['theta']\n",
    "thetas\n",
    "print(thetas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd9a8b",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Note that the shape of ``thetas`` is (N x M).  What are N and M, and what does each mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929b87f",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e68734",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.b Using the output of PyMC3\n",
    "Generate a histogram of all 2,000 posterior samples for $\\theta_2$ (the SAR for Study 2). Use the `plt.hist` function with `density=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5dede6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Create histogram of posterior samples\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7faa5f",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "How do the samples compare to the two different estimates you saw in Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c631e0",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19f5cd",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 2.c Compute Posterior Mean Estimates from Samples\n",
    "\n",
    "Fill in the function that computes posterior mean estimates for each $\\theta_i$ for different parameters $\\alpha, \\beta$ of the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c38a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def empirical_posterior_mean_estimates(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\" \n",
    "    Computes posterior mean estimates of theta_i by performing approximate inference\n",
    "    and then sampling from the posterior distribution:\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Output:\n",
    "        posterior_estimates : (num_studies,) 1-D array of the same length as the \n",
    "            number of studies. posterior_estimates[i] contains the \n",
    "            mean estimate for theta_i based on running MCMC\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    posterior_estimates = ...\n",
    "    return posterior_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c622b95",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be4211",
   "metadata": {},
   "source": [
    "### 2.d Plot the theoretical distribution of the posterior from Question 1 and the empirical distribution of the posterior from Question 2.\n",
    "\n",
    "Make a 4x3 plot such that each subplot corresponds to a study. \n",
    "\n",
    "Each subplot should contain 2 curves and a frequency histogram:\n",
    "- The PDF of the prior distribution of $\\theta_i$\n",
    "- The PDF of the true posterior distribution $\\theta_i|X_i$ computed in closed form, as in Q.1\n",
    "- The histogram of posterior samples of $\\theta_i|X_i$ computed in Q.2\n",
    "\n",
    "Make sure that you properly label each curve and histogram and give each subplot a meaningful title.\n",
    "\n",
    "To give you a mental image of what we have in mind here is a sample subplot. Don't worry if the colors in yours are different.\n",
    "\n",
    "![](sample_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_densities(alpha_value, beta_value, study_df = study_df): \n",
    "    \"\"\"\n",
    "    Plots for each study the prior distribution, true posterior,\n",
    "    and histogram of posterior samples using MCMC\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Outputs:\n",
    "        fig : Figure with 12 subplots\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(4, 3)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    \n",
    "    theta = np.arange(0, 1.01, 0.01)\n",
    "    prior = beta.pdf(theta, alpha_value, beta_value)\n",
    "    \n",
    "    model, trace = approximate_inference_MCMC(alpha_value, beta_value, study_df) \n",
    "    samples = trace['theta'] \n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            idx = 3*i+ j\n",
    "            X_i = study_df.loc[idx, 'X']\n",
    "            N_i = study_df.loc[idx, 'N']\n",
    "            study_name = f'Study {idx}'\n",
    "            true_posterior = beta.pdf(theta, alpha_value+X_i, beta_value+N_i-X_i) \n",
    "            \n",
    "            ax = axs[i, j]\n",
    "            ax.plot(theta, prior, label = 'Prior')\n",
    "            ax.plot(theta, true_posterior, label = \"Theoretical Posterior\")\n",
    "            ax.hist(samples[:,idx], label = \"Empirical Posterior\", density=True, alpha = 0.7)\n",
    "            ax.set_title(study_name)\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32043037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a weak prior\n",
    "fig1 = plot_densities(2, 4, study_df = study_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a strong prior\n",
    "fig2 = plot_densities(20, 40, study_df = study_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf3c1c",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2.d (i) Compare the curve of the theoretical distribution with the histogram of samples from the empirical posterior. Are they similar or different? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fa26d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cb26a",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2.d (ii) Compare the two figures corresponding to 'weak' prior $\\theta_i \\sim Beta(2,4)$ and 'strong' prior  $\\theta_i \\sim Beta(20,40)$. How are they different? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8cd20",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841402e3",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### 2.e Approximate Inference for a More Complex Model\n",
    "\n",
    "The previous 2 parts served as a sanity check that the approximate inference techniques used by PyMC3 can approximate the theoretical posterior. The usefulness of such packages becomes apparent when we are dealing with more complex models that don't have conjugacy properties.\n",
    "\n",
    "Consider the following graphical model:\n",
    "\n",
    "![](model_asymptomatic.png)\n",
    "\n",
    "Recent studies have shown that a large fraction of COVID cases do not show symptoms, but all of the studies considered here tested only symptomatic cases. This means that the probability of testing positive (which what we observe) isn't the same as the SAR $\\theta_i$! \n",
    "\n",
    "The estimates of the asymptomatic rate fall in the range $[0.18, 0.43]$. We assume a prior $A\\sim Uniform(0.18, 0.43)$. This means that the probability that a person in a study tests positive is really $\\theta_i*(1-A)$. Hence:\n",
    "\n",
    "$$X_i|\\theta_i, A \\sim Binomial(N_i, \\theta_i\\cdot (1 - A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19e506",
   "metadata": {},
   "source": [
    "#### Complete the `approximate_inference_asympotmatic_MCMC` function to add dependence on the asymptomatic rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963ee7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def approximate_inference_asympotmatic_MCMC(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\"\n",
    "    Creates and fits a PyMC3 model corresponding to the graphical model above\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "    \"\"\"\n",
    "    with pm.Model() as model:\n",
    "        theta = ...\n",
    "        A = ...\n",
    "        X = ...\n",
    "        \n",
    "        trace = pm.sample(500, tune=1000, target_accept=0.95, return_inferencedata=False, progressbar=False)\n",
    "    return (model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50344696",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2e_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d0967",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Notice that the `trace` now contains samples for both `theta` and `A`!\n",
    "\n",
    "Plot a histogram of the posterior estimates for $A$ if $\\alpha=5$ and $\\beta=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a612df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, trace = approximate_inference_asympotmatic_MCMC(5, 10)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b58a8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Assuming the model we defined is correct, what can you conclude about the asymptomatic rate $A$ based on the studies and the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb034325",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab297345",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Nice Job! You've reached the end of the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6160db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('baby_donkey.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fdec1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbe1bf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23161c",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> n_test_array = [10, 100, 1000]\n>>> x_test_array = [10, 34, 852]\n>>> res_array = [trivial_theta_estimate(n, x_test_array[i]) for i,n in enumerate(n_test_array)]\n>>> assert np.allclose(res_array, [1.0, 0.34, 0.852])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> N_test = 100\n>>> X_test = 20\n>>> alpha_test_array = [1, 10, 100]\n>>> beta_test_array = [1, 10, 100]\n>>> inputs = list(itertools.product(alpha_test_array, beta_test_array))\n>>> outputs = [posterior_mean_estimate(N_test, X_test, *inp) for inp in inputs]\n>>> assert np.allclose(outputs, [0.20588235294117646,\n...  0.1891891891891892,\n...  0.1044776119402985,\n...  0.2702702702702703,\n...  0.25,\n...  0.14285714285714285,\n...  0.5970149253731343,\n...  0.5714285714285714,\n...  0.4])\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a_i": {
     "name": "q2a_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> logger.setLevel(logging.ERROR)\n>>> model, trace = approximate_inference_MCMC(10, 20)\n>>> thetas = trace['theta']\n>>> logger.setLevel(logging.INFO)\n>>> assert thetas.shape == (2000,12)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> logger.setLevel(logging.ERROR)\n>>> nreps = 2\n>>> \n>>> answers = [[0.3],\n...              [0.26],\n...              [0.34],\n...              [0.31],\n...              [0.29],\n...              [0.33],\n...              [0.32, 0.33],\n...              [0.28],\n...              [0.23],\n...              [0.37],\n...              [0.43],\n...              [0.35]]\n>>> \n>>> checks = []\n>>> for _ in range(nreps):\n...     posterior_estimates_test = empirical_posterior_mean_estimates(10,25)\n...     checks.append([np.round(posterior_estimates_test[i],2) in answers[i] for i in range(12)])\n>>> logger.setLevel(logging.INFO)\n>>> assert np.any(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2e_i": {
     "name": "q2e_i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nreps = 2\n>>> logger.setLevel(logging.ERROR)\n>>> answers = [0.38, 0.3, 0.44, 0.38, 0.36, 0.42, 0.42, 0.36, 0.28, 0.48, 0.56, 0.44]\n>>> checks = []\n>>> for _ in range(nreps):\n...     model_test, trace_test = approximate_inference_asympotmatic_MCMC(5, 10)\n...     post_samples_test = trace_test['theta']\n...     estimates = np.mean(post_samples_test, axis = 0)\n...     rounded_estimates = np.round(estimates / 2, 2) * 2\n...     checks.append(np.all([rounded_estimates[i] == answers[i] for i in range(len(rounded_estimates))]))\n>>> logger.setLevel(logging.INFO)\n>>> assert np.any(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
